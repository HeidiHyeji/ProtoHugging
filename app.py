import streamlit as st
import duckdb
import pandas as pd
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain,RetrievalQA
from langchain.chains.llm import LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
import os
from dotenv import load_dotenv  # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œë¥¼ ìœ„í•œ ëª¨ë“ˆ

load_dotenv()

# ğŸ”¹ OpenAI API ì„¤ì •
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°

# ğŸ”¹ DuckDB ì´ˆê¸°í™”
db = duckdb.connect(":memory:")
db.execute("CREATE TABLE users (id INTEGER, name TEXT, age INTEGER);")
db.execute("INSERT INTO users VALUES (1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 35);")

# ğŸ”¹ DuckDB ë°ì´í„°ë¥¼ RAG ë°©ì‹ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ Vector DBì— ì €ì¥
def load_duckdb_data():
    df = db.execute("SELECT * FROM users").fetchdf()
    data_texts = [f"ID: {row['id']}, Name: {row['name']}, Age: {row['age']}" for _, row in df.iterrows()]
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vector_store = FAISS.from_texts(data_texts, embeddings)
    return vector_store

vector_db = load_duckdb_data()
retriever = vector_db.as_retriever()

# ğŸ”¹ LangChain ê¸°ë°˜ AI ëª¨ë¸ ì„¤ì •

llm = ChatOpenAI(
    openai_api_key=OPENAI_API_KEY,
    model="gpt-4",  # GPT-3.5 ëª¨ë¸ ì‚¬ìš©
    temperature=0.1,  # ë‚®ì€ temperatureë¡œ ì¼ê´€ëœ ì¶œë ¥ ìƒì„±
    max_tokens=1000,  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ìš”ì•½ì„ ìœ„í•œ í† í° ìˆ˜ ì„¤ì •
    streaming = True
)


template = """""Please use the following context to answer the question at the end. Return the SQL query statement that fits the MySQL grammar for SQL search at the end of the answer.\
    1. If you don't know the answer, don't tell me you don't know and try to make\
    2. Use up to three sentences. Keep your answers as concise as possible.\
    3. Please answer in Korean and keep your SQL query grammar in English. Be sure to follow the SQL grammar. 
    4. Keep the capital letter, column name of the context in your SQL query grammar\
    5. Always "\n"\nìŠ¤ë§ˆíŠ¸ë¦¬ì˜¨ Partner, thank you for asking!!ğŸ‘" \n"\
    
{context}
Question: {question}
Helpful Answer:"""

QA_CHAIN_PROMPT = PromptTemplate(input_variables=['context', 'question'],
                                 template= template)
question_generator = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  # ğŸš« í•„ìˆ˜ í•„ë“œ ì„¤ì •

memory = ConversationBufferMemory(
    memory_key='chat_history',
    return_messages=True
)
# qa_chain = RetrievalQA.from_chain_type(
#     llm,
#     retriever = retriever,
#     return_source_documents = True,
#     chain_type_kwargs={'prompt': QA_CHAIN_PROMPT}
    
# )
qa_chain = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory = memory,
    combine_docs_chain_kwargs={'prompt': QA_CHAIN_PROMPT}
)
# ğŸ”¹ Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide")

### ğŸ”¹ UI ìƒíƒœ ë³€ìˆ˜
if "explorer_visible" not in st.session_state:
    st.session_state.explorer_visible = True
if "ai_chat_visible" not in st.session_state:
    st.session_state.ai_chat_visible = True
# ğŸ”¹ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

### ğŸ”¹ ë²„íŠ¼ UI ì¶”ê°€
col_title, col_buttons = st.columns([3, 1])  

with col_title:
    st.markdown("<h1 style='text-align: center;'>AIìŠ¤ë§›ë¦¬ì˜¨</h1>", unsafe_allow_html=True)

with col_buttons:
    st.markdown("<div style='text-align: right;'>", unsafe_allow_html=True)
    if st.button("ğŸ“‚ íƒìƒ‰ê¸° ì—´ê¸° / ë‹«ê¸°"):
        st.session_state.explorer_visible = not st.session_state.explorer_visible  # ë²„íŠ¼ í† ê¸€
    if st.button("ğŸ¤– AIìŠ¤ë§›ë¦¬ì˜¨"):
        st.session_state.ai_chat_visible = not st.session_state.ai_chat_visible  # ë²„íŠ¼ í† ê¸€
    st.markdown("</div>", unsafe_allow_html=True)

# ğŸ”¹ ì¤‘ì•™ SQL ì‹¤í–‰ ë ˆì´ì•„ì›ƒ (ì¢Œì¸¡: íƒìƒ‰ê¸° / ì¤‘ì•™: SQL ì‹¤í–‰ + ì°¨íŠ¸ / ìš°ì¸¡: AI ëŒ€í™”ì°½)
col1, col2, col3 = st.columns([
    1 if st.session_state.explorer_visible else 0.1,  # íƒìƒ‰ê¸° í¬ê¸° ì¡°ì ˆ
    3 if not (st.session_state.explorer_visible or st.session_state.ai_chat_visible) else 2,  # SQL ì‹¤í–‰ + ì°¨íŠ¸
    1 if st.session_state.ai_chat_visible else 0.1   # AI ëŒ€í™”ì°½ í¬ê¸° ì¡°ì ˆ
])

### ğŸ”¹ ì¢Œì¸¡: íƒìƒ‰ê¸° (DuckDB í…Œì´ë¸” ì¡°íšŒ)
if st.session_state.explorer_visible:
    with col1:
        st.header("ğŸ“‚ ë°ì´í„° íƒìƒ‰ê¸°")
        tables = db.execute("SHOW TABLES").fetchall()
        selected_table = st.selectbox("í…Œì´ë¸” ì„ íƒ", [t[0] for t in tables])
        if st.button("í…Œì´ë¸” ì¡°íšŒ"):
            result = db.execute(f"SELECT * FROM {selected_table}").fetchdf()
            st.dataframe(result)

### ğŸ”¹ ì¤‘ì•™: SQL ì‹¤í–‰ + ì‹œê°í™”
with col2:
    st.header("ğŸ“ SQL ì‹¤í–‰")
    user_sql = st.text_area("SQL ë¬¸ ì…ë ¥", "SELECT * FROM users;")
    if st.button("ì‹¤í–‰"):
        try:
            result = db.execute(user_sql).fetchdf()
            st.dataframe(result)

            # ğŸ”¹ ìë™ ì°¨íŠ¸ ìƒì„± ë¡œì§ ì¶”ê°€
            if len(result.columns) > 1:
                st.subheader("ğŸ“Š ë°ì´í„° ì‹œê°í™”")
                numeric_columns = result.select_dtypes(include=['int', 'float']).columns.tolist()

                if len(numeric_columns) >= 2:
                    st.line_chart(result.set_index(numeric_columns[0])[numeric_columns[1]])
                    st.bar_chart(result.set_index(numeric_columns[0])[numeric_columns[1]])
                    st.scatter_chart(result.set_index(numeric_columns[0])[numeric_columns[1]])
                else:
                    st.warning("ğŸ“Œ ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•´ ìµœì†Œ ë‘ ê°œì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        except Exception as e:
            st.error(f"SQL ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")

# ğŸ”¹ ëŒ€í™” ë‚´ì—­ ì¶œë ¥
if st.session_state.ai_chat_visible:
    with col3:
        st.header("ğŸ’¬ AI ìŠ¤ë§›ë¦¬ì˜¨")

        # ğŸ”¹ ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
        for chat in st.session_state.chat_history:
            with st.chat_message(chat["role"]):
                st.markdown(chat["message"])

        # ğŸ”¹ ì…ë ¥ì°½ì„ ë¸Œë¼ìš°ì € ê°€ì¥ ì•„ë˜ì— ë°°ì¹˜
        user_query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", key="user_query", placeholder="30ì‚´ ì‚¬ëŒ ì°¾ì•„ì¤˜")

# ğŸ”¹ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ ì‹¤í–‰
if user_query:
    with col3:
        with st.chat_message("user"):
            st.markdown(f"**ğŸ™‹â€â™‚ï¸ ì§ˆë¬¸:** {user_query}")

        # ğŸ”¹ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ì¶œë ¥
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            response_text = ""

            for chunk in qa_chain.stream({"question": user_query, "chat_history": st.session_state.chat_history}):
                response_text += chunk["answer"]
                response_placeholder.markdown(response_text + "â–Œ")

            response_placeholder.markdown(response_text)

        # ğŸ”¹ ëŒ€í™” ê¸°ë¡ ì €ì¥
        st.session_state.chat_history.append({"role": "user", "message": user_query})
        st.session_state.chat_history.append({"role": "assistant", "message": response_text})

